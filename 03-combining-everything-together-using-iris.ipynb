{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": "true"
   },
   "source": [
    " # Table of Contents\n",
    "<div class=\"toc\" style=\"margin-top: 1em;\"><ul class=\"toc-item\" id=\"toc-level0\"><li><span><a href=\"http://localhost:8888/notebooks/03-combining-everything-together-using-iris.ipynb#Iris-dataset-classifier\" data-toc-modified-id=\"Iris-dataset-classifier-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Iris dataset classifier</a></span><ul class=\"toc-item\"><li><span><a href=\"http://localhost:8888/notebooks/03-combining-everything-together-using-iris.ipynb#Load-the-iris-data\" data-toc-modified-id=\"Load-the-iris-data-1.1\"><span class=\"toc-item-num\">1.1&nbsp;&nbsp;</span>Load the iris data</a></span></li><li><span><a href=\"http://localhost:8888/notebooks/03-combining-everything-together-using-iris.ipynb#Features\" data-toc-modified-id=\"Features-1.2\"><span class=\"toc-item-num\">1.2&nbsp;&nbsp;</span>Features</a></span></li><li><span><a href=\"http://localhost:8888/notebooks/03-combining-everything-together-using-iris.ipynb#Defining-inputs,-weights,-biases\" data-toc-modified-id=\"Defining-inputs,-weights,-biases-1.3\"><span class=\"toc-item-num\">1.3&nbsp;&nbsp;</span>Defining inputs, weights, biases</a></span></li><li><span><a href=\"http://localhost:8888/notebooks/03-combining-everything-together-using-iris.ipynb#Training-via-backpropagation\" data-toc-modified-id=\"Training-via-backpropagation-1.4\"><span class=\"toc-item-num\">1.4&nbsp;&nbsp;</span>Training via backpropagation</a></span></li><li><span><a href=\"http://localhost:8888/notebooks/03-combining-everything-together-using-iris.ipynb#Training-using-tf.train.GadientDescentOptimizer()\" data-toc-modified-id=\"Training-using-tf.train.GadientDescentOptimizer()-1.5\"><span class=\"toc-item-num\">1.5&nbsp;&nbsp;</span>Training using <code>tf.train.GadientDescentOptimizer()</code></a></span></li><li><span><a href=\"http://localhost:8888/notebooks/03-combining-everything-together-using-iris.ipynb#Add-accuracy-checking-nodes\" data-toc-modified-id=\"Add-accuracy-checking-nodes-1.6\"><span class=\"toc-item-num\">1.6&nbsp;&nbsp;</span>Add accuracy checking nodes</a></span></li><li><span><a href=\"http://localhost:8888/notebooks/03-combining-everything-together-using-iris.ipynb#Logging-configs\" data-toc-modified-id=\"Logging-configs-1.7\"><span class=\"toc-item-num\">1.7&nbsp;&nbsp;</span>Logging configs</a></span></li><li><span><a href=\"http://localhost:8888/notebooks/03-combining-everything-together-using-iris.ipynb#Set-Number-of-cores-to-use\" data-toc-modified-id=\"Set-Number-of-cores-to-use-1.8\"><span class=\"toc-item-num\">1.8&nbsp;&nbsp;</span>Set Number of cores to use</a></span></li><li><span><a href=\"http://localhost:8888/notebooks/03-combining-everything-together-using-iris.ipynb#Initialize-and-Run\" data-toc-modified-id=\"Initialize-and-Run-1.9\"><span class=\"toc-item-num\">1.9&nbsp;&nbsp;</span>Initialize and Run</a></span></li><li><span><a href=\"http://localhost:8888/notebooks/03-combining-everything-together-using-iris.ipynb#Build-The-summary-operation-based-on-the-TF-collection-of-summaries\" data-toc-modified-id=\"Build-The-summary-operation-based-on-the-TF-collection-of-summaries-1.10\"><span class=\"toc-item-num\">1.10&nbsp;&nbsp;</span>Build The summary operation based on the TF collection of summaries</a></span></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Iris dataset classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "matplotlib.style.use('seaborn')\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "import sklearn\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the iris data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "iris = datasets.load_iris()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['sepal length (cm)',\n",
       " 'sepal width (cm)',\n",
       " 'petal length (cm)',\n",
       " 'petal width (cm)']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iris.feature_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iris.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['setosa', 'versicolor', 'virginica'], \n",
       "      dtype='<U10')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iris.target_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(iris.data, iris.target, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to one hot encoding\n",
    "y_train_hot = keras.utils.np_utils.to_categorical(y_train)\n",
    "y_test_hot = keras.utils.np_utils.to_categorical(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.,  1.,  0.])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test_hot[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 5.5,  2.4,  3.7,  1. ])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A number of features, 4 in this example\n",
    "A = X_train.shape[1]\n",
    "\n",
    "# B = 3 species of Iris\n",
    "B = len(y_train_hot[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining inputs, weights, biases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf_in = tf.placeholder(shape=[None, A], dtype=tf.float32)\n",
    "tf_weight = tf.Variable(tf.zeros(shape=[A, B], dtype=tf.float32))\n",
    "tf_bias = tf.Variable(tf.zeros(B), dtype=tf.float32)\n",
    "tf_softmax = tf.nn.softmax(tf.matmul(tf_in, tf_weight) + tf_bias)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training via backpropagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf_softmax_correct = tf.placeholder(shape=[None, B], dtype=tf.float32)\n",
    "tf_cross_entropy = - tf.reduce_sum(tf_softmax_correct * tf.log(tf_softmax))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training using ```tf.train.GadientDescentOptimizer()```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf_train_step = tf.train.GradientDescentOptimizer(0.01).minimize(tf_cross_entropy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add accuracy checking nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf_correct_prediction = tf.equal(tf.argmax(tf_softmax, 1), tf.argmax(tf_softmax_correct, 1))\n",
    "tf_accuracy = tf.reduce_mean(tf.cast(tf_correct_prediction, dtype=tf.float32))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logging configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import shutil, os, sys\n",
    "TMPDIR = './tenIrisSave'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tmp directory did not exist\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    shutil.rmtree(TMPDIR)\n",
    "    \n",
    "except:\n",
    "    print(\"Tmp directory did not exist\")\n",
    "    os.mkdir(TMPDIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Number of cores to use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "NUM_CORES = 4\n",
    "sess = tf.Session(\n",
    "        config=tf.ConfigProto(inter_op_parallelism_threads=NUM_CORES, intra_op_parallelism_threads=NUM_CORES))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize and Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sess = tf.InteractiveSession()\n",
    "init = tf.global_variables_initializer()\n",
    "sess.run(init)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build The summary operation based on the TF collection of summaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./tenIrisSave/logsd/graph.pbtxt'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.train.write_graph(sess.graph_def, TMPDIR + '/logsd', 'graph.pbtxt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'accuracy:0' shape=() dtype=string>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "tf.summary.scalar('Accuracy_', tf_accuracy)\n",
    "tf.summary.histogram('weights', tf_weight)\n",
    "tf.summary.histogram('bias', tf_bias)\n",
    "tf.summary.histogram('softmax', tf_softmax)\n",
    "tf.summary.histogram('accuracy', tf_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_op = tf.summary.merge_all()\n",
    "summary_writer = tf.summary.FileWriter(TMPDIR + '/logs', sess.graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# You must reset, or on 2nd run of iPython/jupyter you'll get errors\n",
    "# ref: http://stackoverflow.com/a/35424017/904032\n",
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# This is for saving all our work\n",
    "saver = tf.train.Saver([tf_weight,tf_bias])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "k = []\n",
    "saved = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Run 1, 0.2888889014720917\n",
      " Run 11, 0.4888888895511627\n",
      " Run 21, 0.31111112236976624\n",
      "Saving\n",
      " Run 31, 0.7111111283302307\n",
      " Run 41, 0.9777777791023254\n",
      " Run 51, 0.7333333492279053\n",
      " Run 61, 0.8666666746139526\n",
      " Run 71, 0.7111111283302307\n",
      " Run 81, 0.9555555582046509\n",
      " Run 91, 0.7111111283302307\n"
     ]
    }
   ],
   "source": [
    "for i in range(100):\n",
    "    sess.run(tf_train_step, \n",
    "            feed_dict={tf_in: X_train,\n",
    "                       tf_softmax_correct: y_train_hot})\n",
    "    \n",
    "    result = sess.run(tf_accuracy, \n",
    "                     feed_dict={tf_in: X_test,\n",
    "                                tf_softmax_correct: y_test_hot})\n",
    "    if i % 10 == 0:\n",
    "        print(\" Run {}, {}\".format(i+1, result))\n",
    "    \n",
    "    \n",
    "    # Create graphs\n",
    "    summary_str = sess.run(summary_op,\n",
    "                          feed_dict={tf_in: X_test,\n",
    "                                     tf_softmax_correct: y_test_hot})\n",
    "    \n",
    "    summary_writer.add_summary(summary_str, i)\n",
    "    \n",
    "    if result == 1 and saved == 0:\n",
    "        saved = 1\n",
    "        print(\"Saving\")\n",
    "        saver.save(sess, './tenIrisSave/saveOne')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Imgur](https://i.imgur.com/nxWggjS.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/json": {
       "Software versions": [
        {
         "module": "Python",
         "version": "3.6.2 64bit [GCC 4.8.2 20140120 (Red Hat 4.8.2-15)]"
        },
        {
         "module": "IPython",
         "version": "6.1.0"
        },
        {
         "module": "OS",
         "version": "Linux 4.4.0 53 generic x86_64 with debian stretch sid"
        },
        {
         "module": "numpy",
         "version": "1.12.1"
        },
        {
         "module": "tensorflow",
         "version": "1.3.0"
        },
        {
         "module": "keras",
         "version": "2.0.6"
        },
        {
         "module": "sklearn",
         "version": "0.19.0"
        }
       ]
      },
      "text/html": [
       "<table><tr><th>Software</th><th>Version</th></tr><tr><td>Python</td><td>3.6.2 64bit [GCC 4.8.2 20140120 (Red Hat 4.8.2-15)]</td></tr><tr><td>IPython</td><td>6.1.0</td></tr><tr><td>OS</td><td>Linux 4.4.0 53 generic x86_64 with debian stretch sid</td></tr><tr><td>numpy</td><td>1.12.1</td></tr><tr><td>tensorflow</td><td>1.3.0</td></tr><tr><td>keras</td><td>2.0.6</td></tr><tr><td>sklearn</td><td>0.19.0</td></tr><tr><td colspan='2'>Sun Sep 17 18:43:15 2017 CDT</td></tr></table>"
      ],
      "text/latex": [
       "\\begin{tabular}{|l|l|}\\hline\n",
       "{\\bf Software} & {\\bf Version} \\\\ \\hline\\hline\n",
       "Python & 3.6.2 64bit [GCC 4.8.2 20140120 (Red Hat 4.8.2-15)] \\\\ \\hline\n",
       "IPython & 6.1.0 \\\\ \\hline\n",
       "OS & Linux 4.4.0 53 generic x86\\_64 with debian stretch sid \\\\ \\hline\n",
       "numpy & 1.12.1 \\\\ \\hline\n",
       "tensorflow & 1.3.0 \\\\ \\hline\n",
       "keras & 2.0.6 \\\\ \\hline\n",
       "sklearn & 0.19.0 \\\\ \\hline\n",
       "\\hline \\multicolumn{2}{|l|}{Sun Sep 17 18:43:15 2017 CDT} \\\\ \\hline\n",
       "\\end{tabular}\n"
      ],
      "text/plain": [
       "Software versions\n",
       "Python 3.6.2 64bit [GCC 4.8.2 20140120 (Red Hat 4.8.2-15)]\n",
       "IPython 6.1.0\n",
       "OS Linux 4.4.0 53 generic x86_64 with debian stretch sid\n",
       "numpy 1.12.1\n",
       "tensorflow 1.3.0\n",
       "keras 2.0.6\n",
       "sklearn 0.19.0\n",
       "Sun Sep 17 18:43:15 2017 CDT"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%load_ext version_information\n",
    "%version_information numpy, tensorflow, keras, sklearn"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
