{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](http://pytorch.org/tutorials/_images/pytorch-logo-flat.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 id=\"tocheading\">Table of Contents</h1>\n",
    "<div id=\"toc\"></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What is PyTorch\n",
    "\n",
    "It's a Python based scientific computing package targeted at two sets of audiences:\n",
    "- A replacement for numpy to use the power of GPUs\n",
    "- a deep learning research platform that provides maximum flexibility and speed.\n",
    "\n",
    "\n",
    "# Tensors\n",
    "\n",
    "Tensors are similar to numpy's numpy's ndarrays, with the addition being that Tensors can also be used on a GPU to accelerate computing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Construct a 5x3 matrix, uninitialized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1.00000e-11 *\n",
      "  0.0000  0.0000 -0.0000\n",
      "  0.0000 -0.0000  0.0000\n",
      "  0.0000  0.0000  0.0000\n",
      "  0.0000 -3.3373  0.0000\n",
      " -3.2683  0.0000 -0.0000\n",
      "[torch.FloatTensor of size 5x3]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "x = torch.Tensor(5, 3)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Construct a randomly initialized matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " 0.6567  0.4359  0.7633\n",
      " 0.5934  0.6095  0.3843\n",
      " 0.0266  0.9580  0.0040\n",
      " 0.9574  0.2764  0.7318\n",
      " 0.5458  0.3304  0.1916\n",
      "[torch.FloatTensor of size 5x3]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "x = torch.rand(5, 3)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get its size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 3])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Operations\n",
    "There are multiple syntaxes for operations. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       " 0.8202  0.2407  0.2008\n",
       " 0.7726  0.2276  0.1646\n",
       " 0.2548  0.6728  0.4171\n",
       " 0.5427  0.1835  0.1407\n",
       " 0.8626  0.1977  0.0360\n",
       "[torch.FloatTensor of size 5x3]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = torch.rand(5,3)\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       " 1.4768  0.6766  0.9641\n",
       " 1.3659  0.8370  0.5488\n",
       " 0.2814  1.6308  0.4211\n",
       " 1.5000  0.4600  0.8725\n",
       " 1.4084  0.5281  0.2276\n",
       "[torch.FloatTensor of size 5x3]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# syntax 1\n",
    "x+y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       " 1.4768  0.6766  0.9641\n",
       " 1.3659  0.8370  0.5488\n",
       " 0.2814  1.6308  0.4211\n",
       " 1.5000  0.4600  0.8725\n",
       " 1.4084  0.5281  0.2276\n",
       "[torch.FloatTensor of size 5x3]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# syntax 2\n",
    "torch.add(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " 1.4768  0.6766  0.9641\n",
      " 1.3659  0.8370  0.5488\n",
      " 0.2814  1.6308  0.4211\n",
      " 1.5000  0.4600  0.8725\n",
      " 1.4084  0.5281  0.2276\n",
      "[torch.FloatTensor of size 5x3]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# syntax 3\n",
    "result = torch.Tensor(5, 3)\n",
    "torch.add(x, y, out=result)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " 1.4768  0.6766  0.9641\n",
      " 1.3659  0.8370  0.5488\n",
      " 0.2814  1.6308  0.4211\n",
      " 1.5000  0.4600  0.8725\n",
      " 1.4084  0.5281  0.2276\n",
      "[torch.FloatTensor of size 5x3]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Addition: in-place\n",
    "y.add_(x)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"color:blue;\">Any operation that mutates a tensor in-place is post-fixed with an _ For example: x.copy_(y), x.t_(), will change x.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can use standard numpy-like indexing with all bells and whistles!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " 0.6567  0.4359\n",
      " 0.5934  0.6095\n",
      " 0.0266  0.9580\n",
      " 0.9574  0.2764\n",
      " 0.5458  0.3304\n",
      "[torch.FloatTensor of size 5x2]\n",
      "\n",
      "\n",
      " 0.4359\n",
      " 0.6095\n",
      " 0.9580\n",
      " 0.2764\n",
      " 0.3304\n",
      "[torch.FloatTensor of size 5]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(x[:,0:2])\n",
    "print(x[:,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Numpy Bridge\n",
    "Converting a torch Tensor to a numpy array and vice versa is a breeze.\n",
    "\n",
    "The torch Tensor and numpy array will share their underlying memory locations, and changing one will change the other.\n",
    "\n",
    "### Converting torch Tensor to numpy Array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       " 1\n",
       " 1\n",
       " 1\n",
       " 1\n",
       " 1\n",
       "[torch.FloatTensor of size 5]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.ones(5)\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.,  1.,  1.,  1.,  1.], dtype=float32)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b = a.numpy()\n",
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " 2\n",
      " 2\n",
      " 2\n",
      " 2\n",
      " 2\n",
      "[torch.FloatTensor of size 5]\n",
      "\n",
      "[ 2.  2.  2.  2.  2.]\n"
     ]
    }
   ],
   "source": [
    "# see how the numpy array change in value.\n",
    "a.add_(1)\n",
    "print(a)\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Converting numpy Array to torch Tensor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.,  1.,  1.,  1.,  1.])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = np.ones(5)\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       " 1\n",
       " 1\n",
       " 1\n",
       " 1\n",
       " 1\n",
       "[torch.DoubleTensor of size 5]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b = torch.from_numpy(a)\n",
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 2.  2.  2.  2.  2.]\n",
      "\n",
      " 2\n",
      " 2\n",
      " 2\n",
      " 2\n",
      " 2\n",
      "[torch.DoubleTensor of size 5]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "np.add(a, 1, out=a)\n",
    "print(a)\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"color:blue;\"> All the Tensors on the CPU except a CharTensor support converting to NumPy and back.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Autograd: automatic differentiation\n",
    "\n",
    "Central to all neural networks in PyTorch is the **```autograd```** package.\n",
    "\n",
    "The **```autograd```** package provides automatic differentiation for all operations on Tensors. It is a define-by-run framework, which means that our backprop is defined by how our code is run, and that every single iteration can be different.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Variable\n",
    " \n",
    "\n",
    "**```autograd.Variable```** is the central class of the package. It warps a Tensor, and supports nearly all of the operations defined on it. Once we finish our computation we can call ```.backward()``` and have all the gradients computed automatically.\n",
    "\n",
    "We can access the raw tensor through ```.data``` attribute, while the gradient with respect to this variable is accumulated into ```.grad```.\n",
    "\n",
    "![Variable](http://pytorch.org/tutorials/_images/Variable.png)\n",
    "\n",
    "There's one more class which is very important for autograd implementation -a ```Function```.\n",
    "\n",
    "Both ```Variable``` and ```Function``` are interconnected and build up an acyclic graph, that encodes a complete history of computation. Each Variable has a ```.creator``` attribute that references a ```Function``` that has created the ```Variable``` (except for Variables created by the user- their```creator is None```).\n",
    "\n",
    "If we want to compute the derivatives, we can call ```.backward()``` on a ```Variable```. If ```Variable``` is a scalar(i.e it holds a one element data), you don't need to specify any arguments to ```backward()```, however if it has more elements, we need to specify a ```grad_output``` argument that is a tensor of matching shape.\n",
    "\n",
    "## Create a Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Variable containing:\n",
       " 1  1\n",
       " 1  1\n",
       "[torch.FloatTensor of size 2x2]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = Variable(torch.ones(2, 2), requires_grad=True)\n",
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Do an Operation of Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Variable containing:\n",
       " 3  3\n",
       " 3  3\n",
       "[torch.FloatTensor of size 2x2]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = x + 2\n",
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "y was created as a result of an operation, so it has a creator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.autograd._functions.basic_ops.AddConstant at 0x7fb66071b308>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.creator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Variable containing:\n",
       " 27  27\n",
       " 27  27\n",
       "[torch.FloatTensor of size 2x2]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Do more operations on y\n",
    "z = y * y * 3\n",
    "z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Variable containing:\n",
       " 27\n",
       "[torch.FloatTensor of size 1]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out = z.mean()\n",
    "out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradients\n",
    "Let's backprop now ```out.backward()``` is equivalent to doing ```out.backward(torch.Tensor([1.0]))```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "out.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Variable containing:\n",
       " 1  1\n",
       " 1  1\n",
       "[torch.FloatTensor of size 2x2]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable containing:\n",
      " 4.5000  4.5000\n",
      " 4.5000  4.5000\n",
      "[torch.FloatTensor of size 2x2]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's call the ```out``` Variable \"0\". We have that\n",
    "\n",
    "$$o = \\frac{1}{4}\\sum _i z_i$$\n",
    "\n",
    "$$z_i = 3(y_i)^2 = 3(x_i+2)^2$$\n",
    "\n",
    "$$z_i |_{x_i=1}=27$$\n",
    "\n",
    "Therefore, $\\frac{\\partial o }{\\partial x_i} = \\frac{3}{2}(x_i + 2)$,\n",
    "\n",
    "hence $$ \\frac{\\partial o }{\\partial x_i} \\mid _{x_i=1}=\\frac{9}{2}=4.5$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Many Crazy things with autograd!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       " 0.5593\n",
       "-0.9787\n",
       "-0.1997\n",
       "[torch.FloatTensor of size 3]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.randn(3)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Variable containing:\n",
       " 0.5593\n",
       "-0.9787\n",
       "-0.1997\n",
       "[torch.FloatTensor of size 3]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = Variable(x, requires_grad=True)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Variable containing:\n",
       " 1.1186\n",
       "-1.9573\n",
       "-0.3994\n",
       "[torch.FloatTensor of size 3]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = x * 2\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable containing:\n",
      " 2.2372\n",
      "-3.9146\n",
      "-0.7987\n",
      "[torch.FloatTensor of size 3]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(y*2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "while y.data.norm() < 1000:\n",
    "    y = y * 2\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable containing:\n",
      "  572.7346\n",
      "-1002.1454\n",
      " -204.4757\n",
      "[torch.FloatTensor of size 3]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable containing:\n",
      "  102.4000\n",
      " 1024.0000\n",
      "    0.1024\n",
      "[torch.FloatTensor of size 3]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "gradients = torch.FloatTensor([0.1, 1.0, 0.0001])\n",
    "y.backward(gradients)\n",
    "\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Networks\n",
    "\n",
    "Neural networks can be constructed using the ```torch.nn``` package.\n",
    "An ```nn.Module``` contains layers, and a method ```forward(input)``` that returns the ```output```.\n",
    "\n",
    "![](http://pytorch.org/tutorials/_images/mnist.png)\n",
    "\n",
    "The above example is a simple feed-forward network. It takes the input, feeds it through several layers one after the other, and then finally gives the output.\n",
    "\n",
    "A typical training procedure for a neural network is as follows:\n",
    "\n",
    "- Define the neural network that has some learnable parameters (or weights)\n",
    "- Iterate over a dataset of inputs\n",
    "- Process input through the network\n",
    "- Compute the loss (how far is the output from being correct)\n",
    "- Propagate gradients back into the network’s parameters\n",
    "- Update the weights of the network, typically using a simple update rule:\n",
    "\n",
    "\n",
    "**```weight = weight + learning_rate * gradient```**\n",
    "\n",
    "## Define the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        # 1 input image channel, 6 output channels, 5x5 square convolution\n",
    "        # kernel\n",
    "        self.conv1 = nn.Conv2d(1, 6, 5)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        \n",
    "        # an affine operation: y = Wx + b\n",
    "        self.fc1 = nn.Linear( 16 * 5 * 5, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Max pooling over a (2, 2) window\n",
    "        x = F.max_pool2d(F.relu(self.conv1(x)), (2, 2))\n",
    "        \n",
    "        # If the size is a square you can only specify a single number\n",
    "        x = F.max_pool2d(F.relu(self.conv2(x)), 2)\n",
    "        x = x.view(-1, self.num_flat_features(x))\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "    \n",
    "    def num_flat_features(self, x):\n",
    "        size = x.size()[1:] # all dimensions except the batch dimension\n",
    "        num_features = 1\n",
    "        for s in size:\n",
    "            num_features *= s\n",
    "            \n",
    "        return num_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net (\n",
      "  (conv1): Conv2d(1, 6, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (fc1): Linear (400 -> 120)\n",
      "  (fc2): Linear (120 -> 84)\n",
      "  (fc3): Linear (84 -> 10)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "net = Net()\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n"
     ]
    }
   ],
   "source": [
    "# The learnable parameters of a model are returned by net.parameters()\n",
    "params = list(net.parameters())\n",
    "print(len(params))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "(0 ,0 ,.,.) = \n",
      " -0.0605 -0.1816  0.1407  0.0594 -0.1860\n",
      " -0.1002 -0.0117 -0.1758  0.1666  0.1814\n",
      "  0.0593  0.0887  0.0529 -0.1395 -0.1210\n",
      " -0.1229 -0.1658 -0.0270 -0.0538 -0.0611\n",
      " -0.0418 -0.0550  0.1182  0.1332  0.0774\n",
      "\n",
      "(1 ,0 ,.,.) = \n",
      " -0.0293 -0.1883 -0.1464  0.0267  0.0999\n",
      " -0.0900  0.0963 -0.0704 -0.1120 -0.0146\n",
      "  0.0556 -0.1904 -0.1599  0.0296  0.0967\n",
      "  0.1023 -0.0296  0.0584  0.0744 -0.1666\n",
      "  0.0867 -0.1514  0.0774 -0.0108 -0.1040\n",
      "\n",
      "(2 ,0 ,.,.) = \n",
      "  0.1234 -0.1291  0.1795  0.0518  0.1523\n",
      "  0.1703 -0.1489 -0.1645 -0.0835  0.0503\n",
      " -0.0880  0.1814  0.1730  0.1775  0.1042\n",
      "  0.0127  0.0426  0.1772 -0.1851  0.0821\n",
      "  0.0260 -0.0925  0.0300 -0.0081  0.1880\n",
      "\n",
      "(3 ,0 ,.,.) = \n",
      "  0.1224  0.0530 -0.0566  0.1859  0.1536\n",
      "  0.0063 -0.1601  0.1565 -0.0758 -0.1363\n",
      "  0.0597  0.0210  0.0658 -0.0396 -0.1442\n",
      "  0.0865 -0.1174 -0.0925 -0.1286 -0.0331\n",
      " -0.1325  0.1843 -0.0025 -0.0286  0.0954\n",
      "\n",
      "(4 ,0 ,.,.) = \n",
      "  0.0550  0.0836 -0.0661 -0.0379  0.1297\n",
      " -0.0696  0.0909  0.1273 -0.1143  0.0456\n",
      "  0.0502 -0.1468  0.1408  0.1362  0.0032\n",
      "  0.0813  0.1490 -0.0237 -0.1342  0.0993\n",
      " -0.0090  0.1211  0.0034 -0.1021  0.0060\n",
      "\n",
      "(5 ,0 ,.,.) = \n",
      "  0.1476 -0.0556 -0.1391 -0.0513  0.1878\n",
      " -0.1793 -0.0934 -0.1629 -0.1717  0.1076\n",
      "  0.1722  0.0593 -0.1679  0.0662  0.0911\n",
      " -0.1730 -0.0231 -0.1852  0.0198 -0.1036\n",
      " -0.0160 -0.0986  0.1527  0.0941  0.0919\n",
      "[torch.FloatTensor of size 6x1x5x5]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(params[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([6, 1, 5, 5])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params[0].size() # Conv1's weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The input to the forward is an ```autograd.Variable```, and so is the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable containing:\n",
      "(0 ,0 ,.,.) = \n",
      "  0.1428 -0.5461 -1.1254  ...  -1.3203  0.8102  0.2453\n",
      "  0.6096  1.1269 -0.8461  ...   1.0527  0.1020 -1.7761\n",
      "  0.3085  0.0338 -1.1594  ...   0.1656  1.1475  1.8065\n",
      "           ...             ⋱             ...          \n",
      "  0.6506  1.5443  2.4143  ...   0.2695  1.3630  1.7906\n",
      "  0.3943  1.1546  0.3025  ...   1.4561 -1.0245 -0.4550\n",
      "  2.6471 -0.0005  1.5843  ...  -0.0015  0.3269 -1.1769\n",
      "[torch.FloatTensor of size 1x1x32x32]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "input = Variable(torch.randn(1, 1, 32, 32))\n",
    "out = net(input)\n",
    "print(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable containing:\n",
      "-0.0821  0.0263  0.0497 -0.1758  0.1649  0.1227  0.1006  0.0651 -0.0411 -0.0163\n",
      "[torch.FloatTensor of size 1x10]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Zero the gradient buffers of all parameters and backprops with random gradients:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "net.zero_grad()\n",
    "out.backward(torch.randn(1, 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable containing:\n",
      "-0.0821  0.0263  0.0497 -0.1758  0.1649  0.1227  0.1006  0.0651 -0.0411 -0.0163\n",
      "[torch.FloatTensor of size 1x10]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Loss Function\n",
    "\n",
    "A loss function takes the (output, target) pair of inputs, and computes a value that estimates how far away the output is from the target.\n",
    "\n",
    "There are several different [**loss function**](http://pytorch.org/docs/nn.html#loss-functions):\n",
    "- [```nn.L1Loss(size_average=True)```](http://pytorch.org/docs/nn.html#torch.nn.L1Loss)\n",
    "- [```nn.MSELoss(size_average=True)```](http://pytorch.org/docs/nn.html#torch.nn.MSELoss)\n",
    "- [```nn.CrossEntropyLoss(weight=None, size_average=True) ```](http://pytorch.org/docs/nn.html#torch.nn.CrossEntropyLoss)\n",
    "- [```nn.NLLLoss(weight=None, size_average=True) ```](http://pytorch.org/docs/nn.html#torch.nn.NLLLoss)\n",
    "- [``` nn.NLLLoss2d(weight=None, size_average=True)```](http://pytorch.org/docs/nn.html#torch.nn.NLLLoss2d)\n",
    "- [```nn.KLDivLoss(weight=None, size_average=True)```](http://pytorch.org/docs/nn.html#torch.nn.KLDivLoss)\n",
    "- [```nn.BCELoss(weight=None, size_average=True) ```](http://pytorch.org/docs/nn.html#torch.nn.BCELoss)\n",
    "- [```nn.MarginRankingLoss(margin=0, size_average=True) ```](http://pytorch.org/docs/nn.html#torch.nn.MarginRankingLoss)\n",
    "\n",
    "- Et cetera\n",
    "\n",
    "\n",
    "A simple loss is: ```nn.MSELoss``` which computes the mean-squared error between the input and the target."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable containing:\n",
      "(0 ,0 ,.,.) = \n",
      "  0.1428 -0.5461 -1.1254  ...  -1.3203  0.8102  0.2453\n",
      "  0.6096  1.1269 -0.8461  ...   1.0527  0.1020 -1.7761\n",
      "  0.3085  0.0338 -1.1594  ...   0.1656  1.1475  1.8065\n",
      "           ...             ⋱             ...          \n",
      "  0.6506  1.5443  2.4143  ...   0.2695  1.3630  1.7906\n",
      "  0.3943  1.1546  0.3025  ...   1.4561 -1.0245 -0.4550\n",
      "  2.6471 -0.0005  1.5843  ...  -0.0015  0.3269 -1.1769\n",
      "[torch.FloatTensor of size 1x1x32x32]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Variable containing:\n",
       "-0.0821  0.0263  0.0497 -0.1758  0.1649  0.1227  0.1006  0.0651 -0.0411 -0.0163\n",
       "[torch.FloatTensor of size 1x10]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output=net(input)\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Variable containing:\n",
       " 0\n",
       " 1\n",
       " 2\n",
       " 3\n",
       " 4\n",
       " 5\n",
       " 6\n",
       " 7\n",
       " 8\n",
       " 9\n",
       "[torch.FloatTensor of size 10]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target = Variable(torch.arange(0, 10))  # a dummy target, for example\n",
    "target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable containing:\n",
      " 28.2188\n",
      "[torch.FloatTensor of size 1]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "criterion = nn.MSELoss()\n",
    "loss = criterion(output, target)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "input -> conv2d -> relu -> maxpool2d -> conv2d -> relu -> maxpool2d\n",
    "      -> view -> linear -> relu -> linear -> relu -> linear\n",
    "      -> MSELoss\n",
    "      -> loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, when we call loss.backward(), the whole graph is differentiated w.r.t. the loss, and all Variables in the graph will have their .grad Variable accumulated with the gradient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<torch.nn._functions.thnn.auto.MSELoss object at 0x7fb6607574d8>\n",
      "<torch.nn._functions.linear.Linear object at 0x7fb660757050>\n",
      "<torch.nn._functions.thnn.auto.Threshold object at 0x7fb66072eed0>\n"
     ]
    }
   ],
   "source": [
    "print(loss.creator)  # MSELoss\n",
    "print(loss.creator.previous_functions[0][0])  # Linear\n",
    "print(loss.creator.previous_functions[0][0].previous_functions[0][0])  # ReLU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backpropagation\n",
    "To backpropagate the error all we have to do is to call ```loss.backward()```. You need to clear the existing gradients though, else gradients will be accumulated to existing gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conv1.bias.grad before backward\n",
      "Variable containing:\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      "[torch.FloatTensor of size 6]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "net.zero_grad()  # zeroes the gradient buffers of all parameters\n",
    "print('conv1.bias.grad before backward')\n",
    "print(net.conv1.bias.grad)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conv1.bias.grad after backward\n",
      "Variable containing:\n",
      " 0.0308\n",
      "-0.2030\n",
      "-0.0105\n",
      "-0.0009\n",
      "-0.0872\n",
      "-0.0341\n",
      "[torch.FloatTensor of size 6]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "loss.backward()\n",
    "print('conv1.bias.grad after backward')\n",
    "print(net.conv1.bias.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Update the weights\n",
    "The simplest update rule used in practice is the Stochastic Gradient Descent(SGD):\n",
    "\n",
    "**```weight = weight - learning_rate * gradient```**\n",
    "\n",
    "PyTorch has ```torch.optim``` that implements  various different update rules such as SGD, Nesterov-SGD, Adam, RMSProp, etc.. Using it is very simple:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "# Create an optimizer\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.01)\n",
    "\n",
    "for _ in range(1000):\n",
    "    \n",
    "    # in training loop:\n",
    "    optimizer.zero_grad()   # zero the gradient buffers\n",
    "    output = net(input)\n",
    "    loss = criterion(output, target)\n",
    "    loss.backward()\n",
    "    optimizer.step()   # Does the update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "$.getScript('https://kmahelona.github.io/ipython_notebook_goodies/ipython_notebook_toc.js')"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%javascript\n",
    "$.getScript('https://kmahelona.github.io/ipython_notebook_goodies/ipython_notebook_toc.js')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
