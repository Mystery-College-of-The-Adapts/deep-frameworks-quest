{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "IPython.notebook.set_autosave_interval(20000)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Autosaving every 20 seconds\n"
     ]
    }
   ],
   "source": [
    "%autosave 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Author: Anderson Banihirwe\n",
    "# License: MIT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](http://deeplearning.net/tutorial/_images/mlp.png)\n",
    "\n",
    "<h1 align=\"center\"> Backpropagation and Multilayer Perceptron</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 id=\"tocheading\">Table of Contents</h1>\n",
    "<div id=\"toc\"></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Goals:\n",
    "- Train a neural network with keras\n",
    "- Implement Gradient Descent in NumPy\n",
    "- Auto-differentiation with TensorFlow\n",
    "\n",
    "# Dataset Introduction\n",
    "\n",
    "We are going to use sklearn-digits: 10 classes Handwritten digits. For more details, go [here](http://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_digits.html#sklearn.datasets.load_digits).\n",
    "\n",
    "Each datapoint is a 8x8 image of a digit:\n",
    "\n",
    "\n",
    "|                   | Description   |\n",
    "|-------------------|---------------|\n",
    "| Classes           | 10            |\n",
    "| Samples per class | ~180          |\n",
    "| Samples total     | 1797          |\n",
    "| Dimensionality    | 64            |\n",
    "| Features          | integers 0-16 |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.datasets import load_digits\n",
    "import sklearn\n",
    "import matplotlib as mpl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The scikit-learn version is 0.18.1.\n",
      "The NumPy version is 1.12.1.\n",
      "The Matplotlib version is 2.0.1.\n"
     ]
    }
   ],
   "source": [
    "print('The scikit-learn version is {}.'.format(sklearn.__version__))\n",
    "print('The NumPy version is {}.'.format(np.__version__))\n",
    "print('The Matplotlib version is {}.'.format(mpl.__version__))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialize Random Number Generator\n",
    "\n",
    "Next we need to initialize the random number generator to a constant value (7).\n",
    "\n",
    "*This is important to ensure that the results we achieve from this model can be achieved again precisely. It ensures that the stochastic process of training a neural network model can be reproduced.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# fix random seed for reproducibility\n",
    "seed = 7\n",
    "np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1797, 64)\n",
      "<class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "digits = load_digits()\n",
    "print(digits.data.shape)\n",
    "print(type(digits.data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPgAAAEICAYAAAByNDmmAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f477081a2e8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Display some digit\n",
    "plt.imshow(digits.images[0], cmap=plt.cm.gray_r, interpolation='nearest')\n",
    "plt.title(\"image label: {}\".format(digits.target[0]))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing\n",
    "We are going to do some preprocessing before training our classifier.\n",
    "- Convert our data in an array using ```np.asarray()```.\n",
    "- Split our dataset into training and testing sets with ```train_test_split()```\n",
    "- Feature Scaling using standardization: This is used to bring all values into the same range. Feature standardization makes the values of each feature in the data have zero-mean (when subtracting the mean in the numerator) and unit-variance. $$x'=\\frac{x-\\bar{x}}{\\sigma}$$ Where $x$ is the original feature vector, ${\\bar {x}}$ is the mean of that feature vector, and  $\\sigma$  is its standard deviation.\n",
    "\n",
    " - The motivation behind this:  Standardization of a dataset is a common requirement for many machine learning estimators: they might behave badly if the individual feature do not more or less look like standard normally distributed data (e.g. Gaussian with 0 mean and unit variance). For instance many elements used in the objective function of a learning algorithm (such as the RBF kernel of Support Vector Machines or the L1 and L2 regularizers of linear models) assume that all features are centered around 0 and have variance in the same order. If a feature has a variance that is orders of magnitude larger that others, it might dominate the objective function and make the estimator unable to learn from other features correctly as expected.\n",
    " \n",
    " - Another reason why feature scaling is applied is that [**gradient descent**](https://en.wikipedia.org/wiki/Gradient_descent) converges much faster with feature scaling than without it.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Convert our data in array\n",
    "data = np.asarray(digits.data, dtype='float32')\n",
    "target = np.asarray(digits.target, dtype='int32')\n",
    "\n",
    "# Split our data in training and testing set\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "                        data, target, test_size=0.15, random_state=37)\n",
    "\n",
    "# Feature Standardization mean = 0; standard deviation = 1.0\n",
    "scaler = preprocessing.StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.fit_transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scaler Mean:\n",
      "\n",
      " [  0.00000000e+00   3.33333343e-01   5.27407408e+00   1.17962961e+01\n",
      "   1.18000002e+01   5.62222242e+00   1.44444442e+00   1.88888893e-01\n",
      "   0.00000000e+00   1.99629629e+00   1.05851851e+01   1.22555552e+01\n",
      "   1.03740740e+01   8.01111126e+00   1.86296296e+00   1.59259260e-01\n",
      "   0.00000000e+00   2.51851845e+00   1.02888889e+01   7.68148136e+00\n",
      "   7.01481485e+00   7.62962961e+00   1.98518515e+00   8.14814791e-02\n",
      "   0.00000000e+00   2.36666656e+00   9.35555553e+00   9.32222176e+00\n",
      "   9.61481476e+00   7.44074059e+00   2.59629631e+00   0.00000000e+00\n",
      "   0.00000000e+00   2.21481490e+00   7.57407427e+00   9.02962971e+00\n",
      "   9.90740776e+00   8.47037029e+00   2.96296287e+00   0.00000000e+00\n",
      "   1.11111114e-02   1.65925920e+00   7.06296301e+00   7.22592592e+00\n",
      "   7.79629612e+00   8.07407379e+00   3.28888893e+00   3.33333351e-02\n",
      "   0.00000000e+00   8.22222233e-01   7.58518505e+00   9.39259243e+00\n",
      "   9.68518543e+00   8.65555573e+00   3.48888898e+00   1.92592591e-01\n",
      "   0.00000000e+00   2.81481475e-01   5.67407417e+00   1.22555552e+01\n",
      "   1.19555559e+01   6.44814825e+00   2.02962971e+00   4.88888890e-01]\n",
      "\n",
      "\n",
      " [ 1.          0.88610721  4.84260416  4.17922258  4.16386461  5.56159687\n",
      "  3.47619796  1.18222332  1.          3.05322933  5.46692038  3.86214614\n",
      "  4.88126326  6.01262808  3.80542278  1.02225459  1.          3.45843577\n",
      "  5.27940273  5.78715467  6.1955595   6.14544773  3.70981526  0.50992852\n",
      "  1.          2.96691489  5.97087288  5.87240124  6.26806498  5.78008842\n",
      "  3.92286372  1.          1.          3.2894578   6.15600061  6.29278278\n",
      "  5.9618187   5.89640665  3.49689054  1.          0.18223554  3.05908155\n",
      "  6.60805702  6.30261946  6.16615391  5.85456657  4.34931803  0.28995571\n",
      "  1.          1.92975235  5.71532679  5.18487167  5.22183561  6.02124929\n",
      "  4.78247929  0.91081041  1.          0.89568484  5.24769163  4.33391714\n",
      "  4.73343563  5.76283646  4.30407476  2.25048709]\n"
     ]
    }
   ],
   "source": [
    "print(\"Scaler Mean:\\n\\n\", scaler.mean_)\n",
    "print(\"\\n\\n\", scaler.scale_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's display one of the transformed sample (after feature standardization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPgAAAEXCAYAAACAtInoAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f477020bda0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sample_index = 0\n",
    "plt.imshow(X_train[sample_index].reshape(8, 8), cmap=plt.cm.gray_r, \n",
    "           interpolation=\"nearest\")\n",
    "plt.title(\"Transformed Sample\\n(Standardization)\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Scaler object make it possible to recover the original sample:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPgAAAEICAYAAAByNDmmAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\ngGYAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f47702ab358>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(scaler.inverse_transform(X_train[sample_index]).reshape(8, 8), \n",
    "           cmap=plt.cm.gray_r, interpolation=\"nearest\")\n",
    "plt.title(\"original sample\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train-shape --> (1527, 64)\n",
      "y_train-shape --> (1527,)\n",
      "X_test-shape  --> (270, 64)\n",
      "y_test-shape  --> (270,)\n"
     ]
    }
   ],
   "source": [
    "print(\"X_train-shape --> {}\".format(X_train.shape))\n",
    "print(\"y_train-shape --> {}\".format(y_train.shape))\n",
    "print(\"X_test-shape  --> {}\".format(X_test.shape))\n",
    "print(\"y_test-shape  --> {}\".format(y_test.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feed Forward Neural Network with Keras\n",
    "\n",
    "## Objectives\n",
    "The objectives of this section include:\n",
    "- Building and training a feedforward network using [**```Keras```**](https://keras.io/getting-started/sequential-model-guide/)\n",
    "- Experimenting with different optimizers, activations, size of layers, initialization methods.\n",
    "\n",
    "## 1.Keras Workflow\n",
    "\n",
    "### Encode The Output Variable\n",
    "\n",
    "When modeling multi-class classification problems using neural networks, it is good practice to reshape the output attribute from a vector that contains values for each class value to be a matrix with a boolean for each class value and whether or not a given instance has that class value or not.\n",
    "\n",
    "This is called  [\"one-hot-encoding\"](https://en.wikipedia.org/wiki/One-hot) or creating dummy variables from a categorical variable.\n",
    "\n",
    "For example, in this problem 10 class values are integer 0 to 9.\n",
    "Let's print the first five observations from the training set labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 9, 5, 1, 6], dtype=int32)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, each label is just one integer value. We observe that the first entry is \"Two\". We can turn this into a one-hot encoded binary matrix for each data instance. We can do this by first encoding the labels consistently to integers using the scikit-learn [```class LabelEncoder```](http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.LabelEncoder.html#sklearn.preprocessing.LabelEncoder). Then convert the vector of integers to a one hot encoding using the Keras function [```to_categorical()```](https://keras.io/utils/).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "from keras.utils.np_utils import to_categorical\n",
    "\n",
    "# encode class values as integers\n",
    "encoder = LabelEncoder()\n",
    "encoder.fit(y_train)\n",
    "endoded_Y = encoder.transform(y_train)\n",
    "Y_train = to_categorical(endoded_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  1.],\n",
       "       [ 0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.],\n",
       "       [ 0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_train[0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define The Neural Network Model\n",
    "\n",
    "We can now build and train our feed-forward neural network using the high level API from keras. \n",
    "\n",
    "**Note:** The Keras library provides wrapper classes to allow you to use neural network models developed with Keras in Scikit-learn. \n",
    "\n",
    "There is a [```KerasClassifier```](https://keras.io/scikit-learn-api/) class in Keras that can be used as an Estimator in scikit-learn, the base type of model in the library. The KerasClassifier takes the name of a function as an argument. This function must return the constructed neural network model, ready for training.\n",
    "\n",
    "Below is a function that will create a baseline feed-forward network for digit classification. It creates a simple fully connected network with one hidden layer that contains **100 neurons units**.\n",
    "\n",
    "- First we define the model by stacking layers with the right dimensions\n",
    "- then we define a loss function and plug the SGD optimizer\n",
    "- then we feed the model the training data for fixed number of epochs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keras version is 2.0.4.\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Activation\n",
    "from keras import optimizers\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "print('Keras version is {}.'.format(keras.__version__))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# define some constants\n",
    "N = X_train.shape[1]\n",
    "H = 100\n",
    "K = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "64"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# define baseline mlp model\n",
    "def baseline_mlp_model():\n",
    "    # Create model\n",
    "    model = Sequential()\n",
    "    model.add(Dense(units=H, input_dim=N))\n",
    "    model.add(Activation(\"tanh\"))\n",
    "    model.add(Dense(K))\n",
    "    model.add(Activation(\"softmax\"))\n",
    "    \n",
    "    model.compile(optimizer=optimizers.SGD(lr=0.01),\n",
    "                  loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now create our KerasClassifier for use in scikit-learn.\n",
    "\n",
    "We can also pass arguments in the construction of the KerasClassifier class that will be passed on to the ```fit()``` function internally used to train the neural network. Here, we pass the number of epochs as 20 and batch size as 32 to use when training the model. Debugging is also turned on when training by setting verbose to 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "estimator = KerasClassifier(build_fn=baseline_mlp_model, \n",
    "                            epochs=15, batch_size= 32, verbose=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate The Model with k-Fold Cross Validation\n",
    "\n",
    "We can now evaluate the neural network model on our training data.\n",
    "\n",
    "The scikit-learn has excellent capability to evaluate models using a suite of techniques. The gold standard for evaluating machine learning models is [**k-fold cross validation**](http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.KFold.html).\n",
    "\n",
    "First we can define the model evaluation procedure. Here, we set the number of folds to be 10 (an excellent default) and to shuffle the data before partitioning it.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "kfold = KFold(n_splits=5, shuffle=True, random_state=seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can evaluate our model (estimator) on our dataset (X_train and Y_train) using a 10-fold cross validation procedure (kfold).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "0s - loss: 2.1110 - acc: 0.2596\n",
      "Epoch 2/15\n",
      "0s - loss: 1.4683 - acc: 0.5725\n",
      "Epoch 3/15\n",
      "0s - loss: 1.1134 - acc: 0.7396\n",
      "Epoch 4/15\n",
      "0s - loss: 0.8966 - acc: 0.8075\n",
      "Epoch 5/15\n",
      "0s - loss: 0.7542 - acc: 0.8313\n",
      "Epoch 6/15\n",
      "0s - loss: 0.6524 - acc: 0.8509\n",
      "Epoch 7/15\n",
      "0s - loss: 0.5768 - acc: 0.8763\n",
      "Epoch 8/15\n",
      "0s - loss: 0.5184 - acc: 0.8976\n",
      "Epoch 9/15\n",
      "0s - loss: 0.4715 - acc: 0.9091\n",
      "Epoch 10/15\n",
      "0s - loss: 0.4332 - acc: 0.9189\n",
      "Epoch 11/15\n",
      "0s - loss: 0.4018 - acc: 0.9287\n",
      "Epoch 12/15\n",
      "0s - loss: 0.3754 - acc: 0.9361\n",
      "Epoch 13/15\n",
      "0s - loss: 0.3523 - acc: 0.9402\n",
      "Epoch 14/15\n",
      "0s - loss: 0.3325 - acc: 0.9451\n",
      "Epoch 15/15\n",
      "0s - loss: 0.3153 - acc: 0.9468\n",
      "Epoch 1/15\n",
      "0s - loss: 2.0544 - acc: 0.3030\n",
      "Epoch 2/15\n",
      "0s - loss: 1.4092 - acc: 0.6110\n",
      "Epoch 3/15\n",
      "0s - loss: 1.0623 - acc: 0.7600\n",
      "Epoch 4/15\n",
      "0s - loss: 0.8595 - acc: 0.8116\n",
      "Epoch 5/15\n",
      "0s - loss: 0.7273 - acc: 0.8428\n",
      "Epoch 6/15\n",
      "0s - loss: 0.6336 - acc: 0.8698\n",
      "Epoch 7/15\n",
      "0s - loss: 0.5636 - acc: 0.8927\n",
      "Epoch 8/15\n",
      "0s - loss: 0.5095 - acc: 0.9009\n",
      "Epoch 9/15\n",
      "0s - loss: 0.4668 - acc: 0.9075\n",
      "Epoch 10/15\n",
      "0s - loss: 0.4305 - acc: 0.9148\n",
      "Epoch 11/15\n",
      "0s - loss: 0.4007 - acc: 0.9206\n",
      "Epoch 12/15\n",
      "0s - loss: 0.3752 - acc: 0.9271\n",
      "Epoch 13/15\n",
      "0s - loss: 0.3524 - acc: 0.9304\n",
      "Epoch 14/15\n",
      "0s - loss: 0.3332 - acc: 0.9337\n",
      "Epoch 15/15\n",
      "0s - loss: 0.3162 - acc: 0.9386\n",
      "Epoch 1/15\n",
      "0s - loss: 1.9864 - acc: 0.3159\n",
      "Epoch 2/15\n",
      "0s - loss: 1.3735 - acc: 0.6915\n",
      "Epoch 3/15\n",
      "0s - loss: 1.0415 - acc: 0.7848\n",
      "Epoch 4/15\n",
      "0s - loss: 0.8464 - acc: 0.8314\n",
      "Epoch 5/15\n",
      "0s - loss: 0.7199 - acc: 0.8543\n",
      "Epoch 6/15\n",
      "0s - loss: 0.6284 - acc: 0.8740\n",
      "Epoch 7/15\n",
      "0s - loss: 0.5615 - acc: 0.8879\n",
      "Epoch 8/15\n",
      "0s - loss: 0.5089 - acc: 0.9010\n",
      "Epoch 9/15\n",
      "0s - loss: 0.4657 - acc: 0.9141\n",
      "Epoch 10/15\n",
      "0s - loss: 0.4305 - acc: 0.9223\n",
      "Epoch 11/15\n",
      "0s - loss: 0.4010 - acc: 0.9296\n",
      "Epoch 12/15\n",
      "0s - loss: 0.3762 - acc: 0.9345\n",
      "Epoch 13/15\n",
      "0s - loss: 0.3547 - acc: 0.9354\n",
      "Epoch 14/15\n",
      "0s - loss: 0.3359 - acc: 0.9378\n",
      "Epoch 15/15\n",
      "0s - loss: 0.3193 - acc: 0.9403\n",
      "Epoch 1/15\n",
      "0s - loss: 2.1060 - acc: 0.2921\n",
      "Epoch 2/15\n",
      "0s - loss: 1.4706 - acc: 0.6129\n",
      "Epoch 3/15\n",
      "0s - loss: 1.1137 - acc: 0.7390\n",
      "Epoch 4/15\n",
      "0s - loss: 0.8991 - acc: 0.7954\n",
      "Epoch 5/15\n",
      "0s - loss: 0.7571 - acc: 0.8421\n",
      "Epoch 6/15\n",
      "0s - loss: 0.6554 - acc: 0.8691\n",
      "Epoch 7/15\n",
      "0s - loss: 0.5799 - acc: 0.8879\n",
      "Epoch 8/15\n",
      "0s - loss: 0.5208 - acc: 0.9034\n",
      "Epoch 9/15\n",
      "0s - loss: 0.4734 - acc: 0.9165\n",
      "Epoch 10/15\n",
      "0s - loss: 0.4343 - acc: 0.9255\n",
      "Epoch 11/15\n",
      "0s - loss: 0.4023 - acc: 0.9296\n",
      "Epoch 12/15\n",
      "0s - loss: 0.3748 - acc: 0.9354\n",
      "Epoch 13/15\n",
      "0s - loss: 0.3515 - acc: 0.9403\n",
      "Epoch 14/15\n",
      "0s - loss: 0.3315 - acc: 0.9411\n",
      "Epoch 15/15\n",
      "0s - loss: 0.3136 - acc: 0.9444\n",
      "Epoch 1/15\n",
      "0s - loss: 2.0646 - acc: 0.3396\n",
      "Epoch 2/15\n",
      "0s - loss: 1.4330 - acc: 0.6031\n",
      "Epoch 3/15\n",
      "0s - loss: 1.0743 - acc: 0.7324\n",
      "Epoch 4/15\n",
      "0s - loss: 0.8575 - acc: 0.8142\n",
      "Epoch 5/15\n",
      "0s - loss: 0.7153 - acc: 0.8552\n",
      "Epoch 6/15\n",
      "0s - loss: 0.6176 - acc: 0.8797\n",
      "Epoch 7/15\n",
      "0s - loss: 0.5449 - acc: 0.8985\n",
      "Epoch 8/15\n",
      "0s - loss: 0.4898 - acc: 0.9100\n",
      "Epoch 9/15\n",
      "0s - loss: 0.4461 - acc: 0.9182\n",
      "Epoch 10/15\n",
      "0s - loss: 0.4098 - acc: 0.9280\n",
      "Epoch 11/15\n",
      "0s - loss: 0.3805 - acc: 0.9321\n",
      "Epoch 12/15\n",
      "0s - loss: 0.3552 - acc: 0.9394\n",
      "Epoch 13/15\n",
      "0s - loss: 0.3338 - acc: 0.9403\n",
      "Epoch 14/15\n",
      "0s - loss: 0.3152 - acc: 0.9419\n",
      "Epoch 15/15\n",
      "0s - loss: 0.2989 - acc: 0.9460\n",
      "\n",
      "\n",
      "Classifier trained in 8.349 seconds\n"
     ]
    }
   ],
   "source": [
    "from time import time \n",
    "t0 = time()\n",
    "\n",
    "results = cross_val_score(estimator, X_train, Y_train, cv=kfold)\n",
    "\n",
    "tt = time() - t0\n",
    "\n",
    "print(\"\\n\\nClassifier trained in {} seconds\".format(round(tt, 3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline MLP: 92.80% (1.13%)\n"
     ]
    }
   ],
   "source": [
    "print(\"Baseline MLP: %.2f%% (%.2f%%)\" % (results.mean()*100, results.std()*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluating the model only takes approximately 38 seconds and returns an object that describes the evaluation of the 10 constructed models for each of the splits of the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Summary\n",
    "\n",
    "In this post you discovered how to develop and evaluate a neural network using the Keras Python library for deep learning.\n",
    "\n",
    "By completing this tutorial, you learned:\n",
    "\n",
    "How to load data and make it available to Keras.\n",
    "How to prepare multi-class classification data for modeling using one hot encoding.\n",
    "How to use Keras neural network models with scikit-learn.\n",
    "How to define a neural network using Keras for multi-class classification.\n",
    "How to evaluate a Keras neural network model using scikit-learn with k-fold cross validation\n",
    "Do you have any questions about deep learning with Keras or this post?\n",
    "\n",
    "Ask your questions in the comments below and I will do my best to answer them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# MLP for Digits Dataset with grid search via sklearn\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Function to create model, required for KerasClassifier\n",
    "def create_model(optimizer='rmsprop', init='glorot_uniform'):\n",
    "    # Create model\n",
    "    model = Sequential()\n",
    "    model.add(Dense(units=100, input_dim=64, kernel_initializer=init, activation='relu'))\n",
    "    model.add(Dense(units=64, kernel_initializer=init, activation='relu'))\n",
    "    model.add(Dense(units=10, kernel_initializer=init, activation='sigmoid'))\n",
    "    # Compile model\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=optimizer, \n",
    "                  metrics=['accuracy'])\n",
    "    \n",
    "    return model\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create model\n",
    "model = KerasClassifier(build_fn=create_model, verbose=0)\n",
    "\n",
    "# Grid search epochs, batch_size and optimizer\n",
    "optimizers = ['rmsprop', 'adam', 'sgd', 'adagrad']\n",
    "init = ['glorot_uniform', 'normal', 'uniform']\n",
    "epochs = [15, 30, 60]\n",
    "batches = [16, 32, 64]\n",
    "param_grid = dict(optimizer=optimizers, epochs=epochs, \n",
    "                  batch_size=batches, init=init)\n",
    "grid = GridSearchCV(estimator=model, param_grid=param_grid)\n",
    "grid_result = grid.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best: 0.981009 using {'epochs': 30, 'optimizer': 'rmsprop', 'batch_size': 64, 'init': 'glorot_uniform'}\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Summarize results\n",
    "print(\"Best: %f using %s\\n\\n\" % (grid_result.best_score_, \n",
    "                                       grid_result.best_params_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.975769 (0.006483) with: {'epochs': 15, 'optimizer': 'rmsprop', 'batch_size': 16, 'init': 'glorot_uniform'}\n",
      "0.973805 (0.004901) with: {'epochs': 15, 'optimizer': 'adam', 'batch_size': 16, 'init': 'glorot_uniform'}\n",
      "0.921415 (0.010018) with: {'epochs': 15, 'optimizer': 'sgd', 'batch_size': 16, 'init': 'glorot_uniform'}\n",
      "0.976424 (0.008335) with: {'epochs': 15, 'optimizer': 'adagrad', 'batch_size': 16, 'init': 'glorot_uniform'}\n",
      "0.971840 (0.001852) with: {'epochs': 15, 'optimizer': 'rmsprop', 'batch_size': 16, 'init': 'normal'}\n",
      "0.970530 (0.002778) with: {'epochs': 15, 'optimizer': 'adam', 'batch_size': 16, 'init': 'normal'}\n",
      "0.659463 (0.055111) with: {'epochs': 15, 'optimizer': 'sgd', 'batch_size': 16, 'init': 'normal'}\n",
      "0.966601 (0.003208) with: {'epochs': 15, 'optimizer': 'adagrad', 'batch_size': 16, 'init': 'normal'}\n",
      "0.963982 (0.003339) with: {'epochs': 15, 'optimizer': 'rmsprop', 'batch_size': 16, 'init': 'uniform'}\n",
      "0.968566 (0.003208) with: {'epochs': 15, 'optimizer': 'adam', 'batch_size': 16, 'init': 'uniform'}\n",
      "0.216765 (0.042633) with: {'epochs': 15, 'optimizer': 'sgd', 'batch_size': 16, 'init': 'uniform'}\n",
      "0.957433 (0.001852) with: {'epochs': 15, 'optimizer': 'adagrad', 'batch_size': 16, 'init': 'uniform'}\n",
      "0.977734 (0.002450) with: {'epochs': 30, 'optimizer': 'rmsprop', 'batch_size': 16, 'init': 'glorot_uniform'}\n",
      "0.975769 (0.003705) with: {'epochs': 30, 'optimizer': 'adam', 'batch_size': 16, 'init': 'glorot_uniform'}\n",
      "0.956778 (0.003208) with: {'epochs': 30, 'optimizer': 'sgd', 'batch_size': 16, 'init': 'glorot_uniform'}\n",
      "0.973805 (0.002450) with: {'epochs': 30, 'optimizer': 'adagrad', 'batch_size': 16, 'init': 'glorot_uniform'}\n",
      "0.975115 (0.003339) with: {'epochs': 30, 'optimizer': 'rmsprop', 'batch_size': 16, 'init': 'normal'}\n",
      "0.976424 (0.002778) with: {'epochs': 30, 'optimizer': 'adam', 'batch_size': 16, 'init': 'normal'}\n",
      "0.821218 (0.006992) with: {'epochs': 30, 'optimizer': 'sgd', 'batch_size': 16, 'init': 'normal'}\n",
      "0.974460 (0.002778) with: {'epochs': 30, 'optimizer': 'adagrad', 'batch_size': 16, 'init': 'normal'}\n",
      "0.971840 (0.003339) with: {'epochs': 30, 'optimizer': 'rmsprop', 'batch_size': 16, 'init': 'uniform'}\n",
      "0.969876 (0.004037) with: {'epochs': 30, 'optimizer': 'adam', 'batch_size': 16, 'init': 'uniform'}\n",
      "0.474132 (0.145828) with: {'epochs': 30, 'optimizer': 'sgd', 'batch_size': 16, 'init': 'uniform'}\n",
      "0.967911 (0.002450) with: {'epochs': 30, 'optimizer': 'adagrad', 'batch_size': 16, 'init': 'uniform'}\n",
      "0.977079 (0.005633) with: {'epochs': 60, 'optimizer': 'rmsprop', 'batch_size': 16, 'init': 'glorot_uniform'}\n",
      "0.976424 (0.005784) with: {'epochs': 60, 'optimizer': 'adam', 'batch_size': 16, 'init': 'glorot_uniform'}\n",
      "0.971185 (0.009261) with: {'epochs': 60, 'optimizer': 'sgd', 'batch_size': 16, 'init': 'glorot_uniform'}\n",
      "0.973805 (0.007581) with: {'epochs': 60, 'optimizer': 'adagrad', 'batch_size': 16, 'init': 'glorot_uniform'}\n",
      "0.977734 (0.000926) with: {'epochs': 60, 'optimizer': 'rmsprop', 'batch_size': 16, 'init': 'normal'}\n",
      "0.975769 (0.003339) with: {'epochs': 60, 'optimizer': 'adam', 'batch_size': 16, 'init': 'normal'}\n",
      "0.957433 (0.002450) with: {'epochs': 60, 'optimizer': 'sgd', 'batch_size': 16, 'init': 'normal'}\n",
      "0.973150 (0.000926) with: {'epochs': 60, 'optimizer': 'adagrad', 'batch_size': 16, 'init': 'normal'}\n",
      "0.976424 (0.000000) with: {'epochs': 60, 'optimizer': 'rmsprop', 'batch_size': 16, 'init': 'uniform'}\n",
      "0.978389 (0.002778) with: {'epochs': 60, 'optimizer': 'adam', 'batch_size': 16, 'init': 'uniform'}\n",
      "0.867714 (0.020936) with: {'epochs': 60, 'optimizer': 'sgd', 'batch_size': 16, 'init': 'uniform'}\n",
      "0.965946 (0.003339) with: {'epochs': 60, 'optimizer': 'adagrad', 'batch_size': 16, 'init': 'uniform'}\n",
      "0.973150 (0.005633) with: {'epochs': 15, 'optimizer': 'rmsprop', 'batch_size': 32, 'init': 'glorot_uniform'}\n",
      "0.971185 (0.006073) with: {'epochs': 15, 'optimizer': 'adam', 'batch_size': 32, 'init': 'glorot_uniform'}\n",
      "0.833661 (0.010681) with: {'epochs': 15, 'optimizer': 'sgd', 'batch_size': 32, 'init': 'glorot_uniform'}\n",
      "0.975769 (0.009669) with: {'epochs': 15, 'optimizer': 'adagrad', 'batch_size': 32, 'init': 'glorot_uniform'}\n",
      "0.972495 (0.004244) with: {'epochs': 15, 'optimizer': 'rmsprop', 'batch_size': 32, 'init': 'normal'}\n",
      "0.973150 (0.000926) with: {'epochs': 15, 'optimizer': 'adam', 'batch_size': 32, 'init': 'normal'}\n",
      "0.461690 (0.020480) with: {'epochs': 15, 'optimizer': 'sgd', 'batch_size': 32, 'init': 'normal'}\n",
      "0.968566 (0.007351) with: {'epochs': 15, 'optimizer': 'adagrad', 'batch_size': 32, 'init': 'normal'}\n",
      "0.967256 (0.001852) with: {'epochs': 15, 'optimizer': 'rmsprop', 'batch_size': 32, 'init': 'uniform'}\n",
      "0.962672 (0.006416) with: {'epochs': 15, 'optimizer': 'adam', 'batch_size': 32, 'init': 'uniform'}\n",
      "0.186640 (0.123663) with: {'epochs': 15, 'optimizer': 'sgd', 'batch_size': 32, 'init': 'uniform'}\n",
      "0.964637 (0.001604) with: {'epochs': 15, 'optimizer': 'adagrad', 'batch_size': 32, 'init': 'uniform'}\n",
      "0.975115 (0.009121) with: {'epochs': 30, 'optimizer': 'rmsprop', 'batch_size': 32, 'init': 'glorot_uniform'}\n",
      "0.973805 (0.005633) with: {'epochs': 30, 'optimizer': 'adam', 'batch_size': 32, 'init': 'glorot_uniform'}\n",
      "0.926654 (0.002450) with: {'epochs': 30, 'optimizer': 'sgd', 'batch_size': 32, 'init': 'glorot_uniform'}\n",
      "0.975115 (0.009801) with: {'epochs': 30, 'optimizer': 'adagrad', 'batch_size': 32, 'init': 'glorot_uniform'}\n",
      "0.974460 (0.004244) with: {'epochs': 30, 'optimizer': 'rmsprop', 'batch_size': 32, 'init': 'normal'}\n",
      "0.975115 (0.004037) with: {'epochs': 30, 'optimizer': 'adam', 'batch_size': 32, 'init': 'normal'}\n",
      "0.569090 (0.113398) with: {'epochs': 30, 'optimizer': 'sgd', 'batch_size': 32, 'init': 'normal'}\n",
      "0.969876 (0.000926) with: {'epochs': 30, 'optimizer': 'adagrad', 'batch_size': 32, 'init': 'normal'}\n",
      "0.973805 (0.005157) with: {'epochs': 30, 'optimizer': 'rmsprop', 'batch_size': 32, 'init': 'uniform'}\n",
      "0.967911 (0.003339) with: {'epochs': 30, 'optimizer': 'adam', 'batch_size': 32, 'init': 'uniform'}\n",
      "0.175508 (0.074971) with: {'epochs': 30, 'optimizer': 'sgd', 'batch_size': 32, 'init': 'uniform'}\n",
      "0.965946 (0.000926) with: {'epochs': 30, 'optimizer': 'adagrad', 'batch_size': 32, 'init': 'uniform'}\n",
      "0.979044 (0.005633) with: {'epochs': 60, 'optimizer': 'rmsprop', 'batch_size': 32, 'init': 'glorot_uniform'}\n",
      "0.975769 (0.006483) with: {'epochs': 60, 'optimizer': 'adam', 'batch_size': 32, 'init': 'glorot_uniform'}\n",
      "0.954813 (0.008931) with: {'epochs': 60, 'optimizer': 'sgd', 'batch_size': 32, 'init': 'glorot_uniform'}\n",
      "0.971840 (0.006073) with: {'epochs': 60, 'optimizer': 'adagrad', 'batch_size': 32, 'init': 'glorot_uniform'}\n",
      "0.973805 (0.002450) with: {'epochs': 60, 'optimizer': 'rmsprop', 'batch_size': 32, 'init': 'normal'}\n",
      "0.977734 (0.001852) with: {'epochs': 60, 'optimizer': 'adam', 'batch_size': 32, 'init': 'normal'}\n",
      "0.826457 (0.004901) with: {'epochs': 60, 'optimizer': 'sgd', 'batch_size': 32, 'init': 'normal'}\n",
      "0.973150 (0.004037) with: {'epochs': 60, 'optimizer': 'adagrad', 'batch_size': 32, 'init': 'normal'}\n",
      "0.974460 (0.003208) with: {'epochs': 60, 'optimizer': 'rmsprop', 'batch_size': 32, 'init': 'uniform'}\n",
      "0.976424 (0.004812) with: {'epochs': 60, 'optimizer': 'adam', 'batch_size': 32, 'init': 'uniform'}\n",
      "0.407990 (0.118165) with: {'epochs': 60, 'optimizer': 'sgd', 'batch_size': 32, 'init': 'uniform'}\n",
      "0.971840 (0.000926) with: {'epochs': 60, 'optimizer': 'adagrad', 'batch_size': 32, 'init': 'uniform'}\n",
      "0.978389 (0.003208) with: {'epochs': 15, 'optimizer': 'rmsprop', 'batch_size': 64, 'init': 'glorot_uniform'}\n",
      "0.967256 (0.008835) with: {'epochs': 15, 'optimizer': 'adam', 'batch_size': 64, 'init': 'glorot_uniform'}\n",
      "0.554682 (0.061405) with: {'epochs': 15, 'optimizer': 'sgd', 'batch_size': 64, 'init': 'glorot_uniform'}\n",
      "0.969876 (0.006073) with: {'epochs': 15, 'optimizer': 'adagrad', 'batch_size': 64, 'init': 'glorot_uniform'}\n",
      "0.971185 (0.000926) with: {'epochs': 15, 'optimizer': 'rmsprop', 'batch_size': 64, 'init': 'normal'}\n",
      "0.963982 (0.006483) with: {'epochs': 15, 'optimizer': 'adam', 'batch_size': 64, 'init': 'normal'}\n",
      "0.259332 (0.034292) with: {'epochs': 15, 'optimizer': 'sgd', 'batch_size': 64, 'init': 'normal'}\n",
      "0.968566 (0.002778) with: {'epochs': 15, 'optimizer': 'adagrad', 'batch_size': 64, 'init': 'normal'}\n",
      "0.958088 (0.006483) with: {'epochs': 15, 'optimizer': 'rmsprop', 'batch_size': 64, 'init': 'uniform'}\n",
      "0.962017 (0.005633) with: {'epochs': 15, 'optimizer': 'adam', 'batch_size': 64, 'init': 'uniform'}\n",
      "0.187295 (0.036096) with: {'epochs': 15, 'optimizer': 'sgd', 'batch_size': 64, 'init': 'uniform'}\n",
      "0.967911 (0.003705) with: {'epochs': 15, 'optimizer': 'adagrad', 'batch_size': 64, 'init': 'uniform'}\n",
      "0.981009 (0.003339) with: {'epochs': 30, 'optimizer': 'rmsprop', 'batch_size': 64, 'init': 'glorot_uniform'}\n",
      "0.972495 (0.004244) with: {'epochs': 30, 'optimizer': 'adam', 'batch_size': 64, 'init': 'glorot_uniform'}\n",
      "0.829077 (0.038499) with: {'epochs': 30, 'optimizer': 'sgd', 'batch_size': 64, 'init': 'glorot_uniform'}\n",
      "0.973150 (0.002450) with: {'epochs': 30, 'optimizer': 'adagrad', 'batch_size': 64, 'init': 'glorot_uniform'}\n",
      "0.975115 (0.001852) with: {'epochs': 30, 'optimizer': 'rmsprop', 'batch_size': 64, 'init': 'normal'}\n",
      "0.969221 (0.004901) with: {'epochs': 30, 'optimizer': 'adam', 'batch_size': 64, 'init': 'normal'}\n",
      "0.401441 (0.038510) with: {'epochs': 30, 'optimizer': 'sgd', 'batch_size': 64, 'init': 'normal'}\n",
      "0.972495 (0.004812) with: {'epochs': 30, 'optimizer': 'adagrad', 'batch_size': 64, 'init': 'normal'}\n",
      "0.966601 (0.002778) with: {'epochs': 30, 'optimizer': 'rmsprop', 'batch_size': 64, 'init': 'uniform'}\n",
      "0.970530 (0.005557) with: {'epochs': 30, 'optimizer': 'adam', 'batch_size': 64, 'init': 'uniform'}\n",
      "0.161755 (0.071240) with: {'epochs': 30, 'optimizer': 'sgd', 'batch_size': 64, 'init': 'uniform'}\n",
      "0.965291 (0.004037) with: {'epochs': 30, 'optimizer': 'adagrad', 'batch_size': 64, 'init': 'uniform'}\n",
      "0.977734 (0.006483) with: {'epochs': 60, 'optimizer': 'rmsprop', 'batch_size': 64, 'init': 'glorot_uniform'}\n",
      "0.971840 (0.007913) with: {'epochs': 60, 'optimizer': 'adam', 'batch_size': 64, 'init': 'glorot_uniform'}\n",
      "0.927963 (0.008835) with: {'epochs': 60, 'optimizer': 'sgd', 'batch_size': 64, 'init': 'glorot_uniform'}\n",
      "0.977734 (0.004037) with: {'epochs': 60, 'optimizer': 'adagrad', 'batch_size': 64, 'init': 'glorot_uniform'}\n",
      "0.973805 (0.004901) with: {'epochs': 60, 'optimizer': 'rmsprop', 'batch_size': 64, 'init': 'normal'}\n",
      "0.975769 (0.001852) with: {'epochs': 60, 'optimizer': 'adam', 'batch_size': 64, 'init': 'normal'}\n",
      "0.637197 (0.052822) with: {'epochs': 60, 'optimizer': 'sgd', 'batch_size': 64, 'init': 'normal'}\n",
      "0.973150 (0.002450) with: {'epochs': 60, 'optimizer': 'adagrad', 'batch_size': 64, 'init': 'normal'}\n",
      "0.975115 (0.003339) with: {'epochs': 60, 'optimizer': 'rmsprop', 'batch_size': 64, 'init': 'uniform'}\n",
      "0.971840 (0.000926) with: {'epochs': 60, 'optimizer': 'adam', 'batch_size': 64, 'init': 'uniform'}\n",
      "0.204977 (0.053931) with: {'epochs': 60, 'optimizer': 'sgd', 'batch_size': 64, 'init': 'uniform'}\n",
      "0.967256 (0.005633) with: {'epochs': 60, 'optimizer': 'adagrad', 'batch_size': 64, 'init': 'uniform'}\n"
     ]
    }
   ],
   "source": [
    "means = grid_result.cv_results_['mean_test_score']\n",
    "stds = grid_result.cv_results_['std_test_score']\n",
    "params = grid_result.cv_results_['params']\n",
    "\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print(\"%f (%f) with: %r\" %(mean, stdev, param))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%javascript\n",
    "$.getScript('https://kmahelona.github.io/ipython_notebook_goodies/ipython_notebook_toc.js')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
